version: '3.8'

services:
  embedding-service:
    build:
      context: .
      dockerfile: docker/embedding-service/Dockerfile
      args:
        # Default to CPU version. For GPU, override with:
        # docker-compose build --build-arg PYTORCH_INDEX_URL=https://download.pytorch.org/whl/cu121 embedding-service
        PYTORCH_INDEX_URL: https://download.pytorch.org/whl/cpu
    container_name: vector-db-embedding
    ports:
      - "8001:8001"
    volumes:
      - ./shared:/app/shared:Z
    # Config file is copied into image, no volume mount needed
    environment:
      - CONFIG_PATH=/app/config.yaml
      - HF_HUB_OFFLINE=1
      - MMAP_SHARED_DIR=/app/shared
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://127.0.0.1:8001/health" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - vdb-net
    # Allow scaling independently (can scale GPU instances separately)
    deploy:
      replicas: 1

  indexing-service:
    build:
      context: .
      dockerfile: docker/indexing-service/Dockerfile
    container_name: vector-db-indexing
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data:Z
      - ./src/config.yaml:/app/config.yaml:ro,Z
      - ./shared:/app/shared:Z
    environment:
      - CONFIG_PATH=/app/config.yaml
      - MMAP_SHARED_DIR=/app/shared
    depends_on:
      - embedding-service
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://127.0.0.1:8000/health" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - vdb-net
    # Allow scaling independently
    deploy:
      replicas: 1
networks:
  vdb-net:
    driver: bridge